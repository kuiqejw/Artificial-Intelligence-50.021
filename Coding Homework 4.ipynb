{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, io\n",
    "import skimage.color\n",
    "from torch import nn\n",
    "import xml.etree.ElementTree as ET\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from scipy.misc import imread\n",
    "from collections import OrderedDict\n",
    "import torchvision\n",
    "import skimage\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)]\n",
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)\n",
    "print (torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# torch.utils.data.DataLoader(dataset, batch_size=1, \n",
    "#                             shuffle=False, sampler=None, batch_sampler=None,\n",
    "#                             num_workers=0, collate_fn=<function default_collate>, pin_memory=False, \n",
    "#                             drop_last=False, timeout=0, worker_init_fn=None)\n",
    "class dataclass(Dataset):\n",
    "    def __init__(self,allclasses,imagesDirectory,xmlDirectory,normalize = True,fivecrop=False,is330=False):\n",
    "        f = open(allclasses,\"r\")\n",
    "        self.labels_dict = {}\n",
    "        self.number_labels = {}\n",
    "        self.counter_labels = {}\n",
    "        counter=0\n",
    "        for line in f:\n",
    "            self.labels_dict[line[:9]] = line[10:-1]\n",
    "            self.number_labels[line[10:-1]] = counter\n",
    "            self.counter_labels[counter] =line[10:-1] \n",
    "            counter+=1\n",
    "        self.imageList = os.listdir(imagesDirectory)  # UNSAVED\n",
    "        self.xmlsDirectory = os.listdir(xmlDirectory)  # UNSAVED\n",
    "        if normalize and fivecrop:\n",
    "            self.transforms =  transforms.Compose([transforms.ToPILImage(),transforms.Resize(280),torchvision.transforms.FiveCrop(224),\n",
    "                                                   transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                                                   transforms.Lambda(lambda norms: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])(norm) for norm in norms]))])\n",
    "\n",
    "        elif not normalize:\n",
    "            self.transforms =  transforms.Compose([transforms.ToPILImage(),transforms.Resize(224),transforms.CenterCrop(224),transforms.ToTensor()])\n",
    "        else:\n",
    "            self.transforms =  transforms.Compose([transforms.ToPILImage(),transforms.Resize(224),transforms.CenterCrop(224),transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "        if is330:\n",
    "            self.transforms = transforms.Compose([transforms.ToPILImage(),transforms.Resize(380),torchvision.transforms.FiveCrop(330),\n",
    "                                                   transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                                                   transforms.Lambda(lambda norms: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])(norm) for norm in norms]))])\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return_dict = self.retrieve_xml(self.xmlsDirectory[idx])\n",
    "        #return_dict[\"image\"] = io.imread(\"imagespart/\"+ self.imageList[idx])\n",
    "        holder = skimage.io.imread(\"imagespart/\"+ self.imageList[idx])\n",
    "        return_dict[\"class\"] = self.number_labels[return_dict[\"name\"]]\n",
    "        if(len(holder.shape) == 2):\n",
    "            holder = np.stack((holder,)*3, axis=-1)\n",
    "        return_dict[\"image\"] = holder\n",
    "        return_dict[\"image\"] = self.transforms(return_dict[\"image\"])\n",
    "#         print(\"return_dict['image'].shape\")\n",
    "#         print(return_dict[\"image\"].shape)\n",
    "        #return_dict[\"image\"] = transforms.functional.to_grayscale(return_dict[\"image\"], num_output_channels=1)\n",
    "        \n",
    "        \n",
    "        #return_dict[\"image\"] = self.auto_image_reshape(return_dict[\"image\"],224,return_dict)\n",
    "        \n",
    "        #return_dict[\"image\"] = return_dict[\"image\"]\n",
    "        #.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]) ????\n",
    "        #NORMALISE\n",
    "        \n",
    "        return return_dict\n",
    "         \n",
    "    def __len__(self):\n",
    "            return len(self.imageList)        \n",
    "        \n",
    "    def retrieve_xml(self,reference):\n",
    "        # take a single xml's name and obtain all information from it.\n",
    "        root =ET.parse(\"val/\" + reference).getroot()\n",
    "        dicty = {}\n",
    "        for sizevar in root.findall(\"size\"):\n",
    "            dicty[\"width\"] = sizevar.findall(\"width\")[0].text\n",
    "            dicty[\"height\"] = sizevar.findall(\"height\")[0].text\n",
    "            dicty[\"depth\"] = sizevar.findall(\"depth\")[0].text\n",
    "        for obj in root.findall('object'):\n",
    "            for name in obj.findall('name'):\n",
    "#                 print(name.text)\n",
    "                dicty[\"name\"] = self.labels_dict[name.text]\n",
    "            for pose in obj.findall('pose'):\n",
    "                dicty[\"pose\"] = pose.text\n",
    "            for trunc in obj.findall('truncated'):\n",
    "                dicty[\"trunc\"]  = bool(int(trunc.text))\n",
    "            for difficult in obj.findall('difficult'):\n",
    "                dicty[\"difficult\"] = bool(int(difficult.text))\n",
    "            for bndbox in obj.findall('bndbox' ):\n",
    "                dicty[\"xmin\"] = int(bndbox.findall(\"xmin\")[0].text)\n",
    "                dicty[\"ymin\"] = int(bndbox.findall(\"ymin\")[0].text)\n",
    "                dicty[\"xmax\"] = int(bndbox.findall(\"xmax\")[0].text)\n",
    "                dicty[\"ymax\"] = int(bndbox.findall(\"ymax\")[0].text)\n",
    "        return dicty\n",
    "\n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.eval()\n",
    "    for batch_idx, dict_of_values in enumerate(train_loader):\n",
    "        data = dict_of_values[\"image\"]\n",
    "        \n",
    "        target = dict_of_values[\"class\"]\n",
    "        data, target = data.to(dict_of_values[\"image\"]), target.to(dict_of_values[\"class\"])\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "#         print(output.shape)\n",
    "#         print(target.shape)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    return loss.item()\n",
    "\n",
    "def validation_motion(log_interval, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    occurrence = {}\n",
    "    correctness ={}\n",
    "    setcount=0\n",
    "    with torch.no_grad():\n",
    "        for dict_of_values in test_loader:\n",
    "            data = dict_of_values[\"image\"]\n",
    "            target = dict_of_values[\"class\"]\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            \n",
    "            for kk in range(len(pred)):\n",
    "                if pred[kk] == target[kk]:\n",
    "                    correct+=1\n",
    "                total+=1\n",
    "            setcount+=1\n",
    "            if setcount%10==0:\n",
    "                print(\"Current Set: \" + str(setcount))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correctness,occurrence\n",
    "def validation_motion(log_interval, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    occurrence = {}\n",
    "    correctness ={}\n",
    "    setcount=0\n",
    "    with torch.no_grad():\n",
    "        for dict_of_values in test_loader:\n",
    "            data = dict_of_values[\"image\"]\n",
    "            target = dict_of_values[\"class\"]\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            \n",
    "            for kk in range(len(pred)):\n",
    "                if pred[kk] == target[kk]:\n",
    "                    correct+=1\n",
    "                total+=1\n",
    "            setcount+=1\n",
    "            if setcount%10==0:\n",
    "                print(\"Current Set: \" + str(setcount))\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correctness,occurrence\n",
    "\n",
    "def validation_motion2(log_interval, model, device, test_loader,is5crop=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total=0\n",
    "    occurrence = {}\n",
    "    correctness ={}\n",
    "    setcount=0\n",
    "    with torch.no_grad():\n",
    "        for dict_of_values in test_loader:\n",
    "            data = dict_of_values[\"image\"]\n",
    "            target = dict_of_values[\"class\"]\n",
    "#             print(\"data shape: \" + str(data.shape))\n",
    "            if len(data.shape)>4:\n",
    "                #target = target.view(-1,1).repeat(1,data.shape[1]).view(-1,1)\n",
    "                data = data.view([-1,data.shape[-3],data.shape[-2],data.shape[-1]])\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            if is5crop:\n",
    "                output = output.view(test_loader.batch_size, 5, -1)\n",
    "#                 print(\"output after view = \"+ str(output.shape))\n",
    "                pred = output.mean(dim=1) # get the index of the max log-probability\n",
    "#                 print(\"output mean across dim 1 : \"+ str(pred.shape))\n",
    "                pred = pred.argmax(dim=1, keepdim=True).squeeze(1)\n",
    "#                 print(pred.shape)\n",
    "#                 print(target.shape)\n",
    "            else:\n",
    "                pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            \n",
    "            for kk in range(len(pred)):\n",
    "                if pred[kk] == target[kk]:\n",
    "                    correct+=1\n",
    "                total+=1\n",
    "            setcount+=1\n",
    "            if setcount%10==0:\n",
    "                print(\"Current Set: \" + str(setcount))\n",
    "\n",
    "    test_loss = correct/total\n",
    "    \n",
    "    print('\\n 5 crop Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss, correctness,occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\models\\squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "c:\\users\\user\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\models\\squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without normalise\n",
      "Current Set: 10\n",
      "Current Set: 20\n",
      "Current Set: 30\n",
      "Current Set: 40\n",
      "Current Set: 50\n",
      "Current Set: 60\n",
      "Current Set: 70\n",
      "Current Set: 80\n",
      "Current Set: 90\n",
      "Current Set: 100\n",
      "Current Set: 110\n",
      "Current Set: 120\n",
      "Current Set: 130\n",
      "Current Set: 140\n",
      "Current Set: 150\n",
      "Current Set: 160\n",
      "Current Set: 170\n",
      "Current Set: 180\n",
      "Current Set: 190\n",
      "Current Set: 200\n",
      "Current Set: 210\n",
      "Current Set: 220\n",
      "Current Set: 230\n",
      "Current Set: 240\n",
      "Current Set: 250\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 340/2500 (14%)\n",
      "\n",
      "With normalise\n",
      "Current Set: 10\n",
      "Current Set: 20\n",
      "Current Set: 30\n",
      "Current Set: 40\n",
      "Current Set: 50\n",
      "Current Set: 60\n",
      "Current Set: 70\n",
      "Current Set: 80\n",
      "Current Set: 90\n",
      "Current Set: 100\n",
      "Current Set: 110\n",
      "Current Set: 120\n",
      "Current Set: 130\n",
      "Current Set: 140\n",
      "Current Set: 150\n",
      "Current Set: 160\n",
      "Current Set: 170\n",
      "Current Set: 180\n",
      "Current Set: 190\n",
      "Current Set: 200\n",
      "Current Set: 210\n",
      "Current Set: 220\n",
      "Current Set: 230\n",
      "Current Set: 240\n",
      "Current Set: 250\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 1440/2500 (58%)\n",
      "\n",
      "5 crop with normalise\n",
      "Current Set: 10\n",
      "Current Set: 20\n",
      "Current Set: 30\n",
      "Current Set: 40\n",
      "Current Set: 50\n",
      "Current Set: 60\n",
      "Current Set: 70\n",
      "Current Set: 80\n",
      "Current Set: 90\n",
      "Current Set: 100\n",
      "Current Set: 110\n",
      "Current Set: 120\n",
      "Current Set: 130\n",
      "Current Set: 140\n",
      "Current Set: 150\n",
      "Current Set: 160\n",
      "Current Set: 170\n",
      "Current Set: 180\n",
      "Current Set: 190\n",
      "Current Set: 200\n",
      "Current Set: 210\n",
      "Current Set: 220\n",
      "Current Set: 230\n",
      "Current Set: 240\n",
      "Current Set: 250\n",
      "\n",
      " 5 crop Test set: Average loss: 0.6136, Accuracy: 1534/2500 (61%)\n",
      "\n",
      "squeezenet with 330\n",
      "Current Set: 10\n",
      "Current Set: 20\n",
      "Current Set: 30\n",
      "Current Set: 40\n",
      "Current Set: 50\n",
      "Current Set: 60\n",
      "Current Set: 70\n",
      "Current Set: 80\n",
      "Current Set: 90\n",
      "Current Set: 100\n",
      "Current Set: 110\n",
      "Current Set: 120\n",
      "Current Set: 130\n",
      "Current Set: 140\n",
      "Current Set: 150\n",
      "Current Set: 160\n",
      "Current Set: 170\n",
      "Current Set: 180\n",
      "Current Set: 190\n",
      "Current Set: 200\n",
      "Current Set: 210\n",
      "Current Set: 220\n",
      "Current Set: 230\n",
      "Current Set: 240\n",
      "Current Set: 250\n",
      "\n",
      " 5 crop Test set: Average loss: 0.6028, Accuracy: 1507/2500 (60%)\n",
      "\n",
      "Inception With 330\n",
      "Current Set: 10\n",
      "Current Set: 20\n",
      "Current Set: 30\n",
      "Current Set: 40\n",
      "Current Set: 50\n",
      "Current Set: 60\n",
      "Current Set: 70\n",
      "Current Set: 80\n",
      "Current Set: 90\n",
      "Current Set: 100\n",
      "Current Set: 110\n",
      "Current Set: 120\n",
      "Current Set: 130\n",
      "Current Set: 140\n",
      "Current Set: 150\n",
      "Current Set: 160\n",
      "Current Set: 170\n",
      "Current Set: 180\n",
      "Current Set: 190\n",
      "Current Set: 200\n",
      "Current Set: 210\n",
      "Current Set: 220\n",
      "Current Set: 230\n",
      "Current Set: 240\n",
      "Current Set: 250\n",
      "\n",
      " 5 crop Test set: Average loss: 0.7440, Accuracy: 1860/2500 (74%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Squeeze330(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(Squeeze330,self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.newPool = nn.AdaptiveAvgPool2d((224,224))\n",
    "        self.reference = models.squeezenet1_1(pretrained = True)\n",
    "    def forward(self,x):\n",
    "        x = self.newPool(x)\n",
    "        x = self.reference.features(x)\n",
    "        x = self.reference.classifier(x)\n",
    "        return x.view(x.size(0), self.num_classes)\n",
    "    \n",
    "class Inception330(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n",
    "        super(Inception330, self).__init__()\n",
    "        self.newPool = nn.AdaptiveAvgPool2d((224,224))\n",
    "        self.reference = models.inception_v3(pretrained=True)\n",
    "    def forward(self, x):\n",
    "        features = self.newPool(x)\n",
    "        features = self.reference.forward(features)\n",
    "        return features\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    a = dataclass(\"synset_words.txt\",\"imagespart/\",\"val/\",True,False,False) #with normalise\n",
    "    b = dataclass(\"synset_words.txt\",\"imagespart/\",\"val/\",False,False,False) #without normalise\n",
    "    c = dataclass(\"synset_words.txt\",\"imagespart/\",\"val/\",True,True,False) #normalised, fivecrop\n",
    "    d = dataclass(\"synset_words.txt\",\"imagespart/\",\"val/\",True,True,True) #normalised, fivecrop, 330 input\n",
    "    image_dataload1 = DataLoader(a,batch_size=10,shuffle=True,num_workers =0,timeout=4)\n",
    "    image_dataload2 = DataLoader(b,batch_size=10,shuffle=True,num_workers =0,timeout=4)\n",
    "    image_dataload3 = DataLoader(c,batch_size=10,shuffle=True,num_workers =0,timeout=4)\n",
    "    image_dataload4 = DataLoader(d,batch_size=10,shuffle=True,num_workers =0,timeout=4)\n",
    "    device = torch.device(\"cpu\")\n",
    "    squeezenet = models.squeezenet1_1(pretrained = True)\n",
    "    learning_rate = 0.01\n",
    "    momentum_mod = 0.001\n",
    "    optimizer = optim.SGD(squeezenet.parameters(), lr=learning_rate, momentum=momentum_mod)\n",
    "    log_interval = 100\n",
    "    print(\"without normalise\")\n",
    "    validation_motion(log_interval, squeezenet, device, image_dataload2)\n",
    "    print(\"With normalise\")\n",
    "    validation_motion(log_interval, squeezenet, device, image_dataload1)\n",
    "    print(\"5 crop with normalise\")\n",
    "    validation_motion2(log_interval, squeezenet, device, image_dataload3,True)\n",
    "    print(\"squeezenet with 330\")\n",
    "    validation_motion2(log_interval, Squeeze330(), device, image_dataload4,True)\n",
    "    print(\"Inception With 330\")\n",
    "    validation_motion2(log_interval, Inception330(), device, image_dataload4,True)\n",
    "    \n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "#     def auto_image_reshape(self,image,target_smalls,boundcontainer):\n",
    "#         # reshape your image and redefine boundaries.\n",
    "#         if (image.shape[0]>target_smalls or image.shape[1]>target_smalls) and  (not image.shape[0]<=target_smalls and not image.shape[1]<target_smalls):\n",
    "#             # if either is greater, and there is not a side that is <=target_smalls\n",
    "#             if image.shape[0]<image.shape[1]:\n",
    "# #                 print(image.shape)\n",
    "#                 image_rescaled = transform.rescale(image,target_smalls/image.shape[0], anti_aliasing=False,multichannel = True)\n",
    "#             else:\n",
    "# #                 print(image.shape)\n",
    "#                 image_rescaled = transform.rescale(image,target_smalls/image.shape[1], anti_aliasing=False,multichannel = True)\n",
    "# #                 print(image_rescaled.shape)\n",
    "#             #resize boundaries accordingly\n",
    "#             boundcontainer[\"xmin\"] = int(boundcontainer[\"xmin\"]*image_rescaled.shape[0]/image.shape[0])\n",
    "#             boundcontainer[\"xmax\"] = int(boundcontainer[\"xmax\"]*image_rescaled.shape[0]/image.shape[0])\n",
    "#             boundcontainer[\"ymin\"] = int(boundcontainer[\"ymin\"]*image_rescaled.shape[1]/image.shape[1])\n",
    "#             boundcontainer[\"ymax\"] = int(boundcontainer[\"ymax\"]*image_rescaled.shape[1]/image.shape[1])\n",
    "#         else:\n",
    "#             return image\n",
    "#         return image_rescaled\n",
    "    \n",
    "#     def cropped_bounds(self,bounds,xmin,xmax,ymin,ymax):\n",
    "#         # redefine bounds of label after a crop\n",
    "#         return_xmin =0\n",
    "#         return_xmax =0\n",
    "#         return_ymin =0\n",
    "#         return_ymax =0\n",
    "#         if bounds[\"xmin\"]<xmin:\n",
    "#             return_xmin = bounds[\"xmin\"]\n",
    "#         else:\n",
    "#             return_xmin = xmin\n",
    "#         if bounds[\"xmax\"]<xmax:\n",
    "#             return_xmax = bounds[\"xmin\"]\n",
    "#         else:\n",
    "#             return_xmax = xmax\n",
    "#         if bounds[\"ymin\"]<ymin:\n",
    "#             return_ymin =bounds[\"ymin\"]\n",
    "#         else:\n",
    "#             return_ymin = ymin\n",
    "#         if bounds[\"ymax\"]<ymax:\n",
    "#             return_ymax =bounds[\"ymax\"]\n",
    "#         else:\n",
    "#             return_ymax = ymax\n",
    "#         return return_xmin,return_xmax,return_ymin,return_ymax\n",
    "    \n",
    "#     def five_crop(self,image,bounds):\n",
    "#         #return all 5 crops, with bounds in a dictionary.\n",
    "#         side1 = image.shape[0]\n",
    "#         side2 = image.shape[1]\n",
    "#         topleftdict = {}\n",
    "#         toprightdict={}\n",
    "#         botleftdict={}\n",
    "#         botrightdict={}\n",
    "#         centraldict = {}\n",
    "#         #assume 3 channels in general.\n",
    "#         topleft = plt.imshow(image[:int(side1/2),:int(side2/2),:])\n",
    "#         xmin, xmax, ymin, ymax = cropped_bounds(bounds,0,int(side1/2),0, int(side2/2))\n",
    "#         topleftdict[\"xmin\"] = xmin\n",
    "#         topleftdict[\"xmax\"] = xmax\n",
    "#         topleftdict[\"ymin\"] = ymin\n",
    "#         topleftdict[\"ymax\"] = ymax\n",
    "#         plt.show()\n",
    "#         topright = plt.imshow(image[int(side1/2):,:int(side2/2),:])\n",
    "#         xmin, xmax, ymin, ymax = cropped_bounds(bounds,int(side1/2),side1,0, int(side2/2))\n",
    "#         toprightdict[\"xmin\"] = xmin\n",
    "#         toprightdict[\"xmax\"] = xmax\n",
    "#         toprightdict[\"ymin\"] = ymin\n",
    "#         toprightdict[\"ymax\"] = ymax\n",
    "#         plt.show()\n",
    "#         botleft = plt.imshow(image[:int(side1/2),int(side2/2):,:])\n",
    "#         xmin, xmax, ymin, ymax = cropped_bounds(bounds,0,int(side1/2),int(side2/2), side2)\n",
    "#         botleftdict[\"xmin\"] = xmin\n",
    "#         botleftdict[\"xmax\"] = xmax\n",
    "#         botleftdict[\"ymin\"] = ymin\n",
    "#         botleftdict[\"ymax\"] = ymax\n",
    "#         plt.show()\n",
    "#         botright = plt.imshow(image[int(side1/2):,int(side2/2):,:])\n",
    "#         xmin, xmax, ymin, ymax = cropped_bounds(bounds,int(side1/2),side1,int(side2/2), side2)\n",
    "#         botrightdict[\"xmin\"] = xmin\n",
    "#         botrightdict[\"xmax\"] = xmax\n",
    "#         botrightdict[\"ymin\"] = ymin\n",
    "#         botrightdict[\"ymax\"] = ymax\n",
    "#         plt.show()\n",
    "#         central = plt.imshow(image[int(side1/4):int(side1* 3/4),int(side2/4):int(side2/4*3),:])\n",
    "#         xmin, xmax, ymin, ymax = cropped_bounds(bounds,int(side1/4),int(side1*3/4),int(side2/4), int(side2/4*3))\n",
    "#         centraldict[\"xmin\"] = xmin\n",
    "#         centraldict[\"xmax\"] = xmax\n",
    "#         centraldict[\"ymin\"] = ymin\n",
    "#         centraldict[\"ymax\"] = ymax\n",
    "#         plt.show()\n",
    "#         return topleft,topleftdict,topright,toprightdict,botleft,botleftdict,botright,botrightdict,central,centraldict\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "#     def find_center(self,image):\n",
    "#         # find the center of an image after it was resized to 224 and something. or something and 224\n",
    "#         if image.shape[0]==224:\n",
    "#             temp = int(image.shape[1]/2)\n",
    "#             return_image = image[:,temp-112:temp+112,:]\n",
    "#         else: #image.shape[1]==224\n",
    "#             temp = int(image.shape[0]/2)\n",
    "#             return_image = image[temp-112:temp+112,:,:]\n",
    "# #         plt.imshow(return_image)\n",
    "# #         print(return_image)\n",
    "# #         plt.show()\n",
    "#         return return_image\n",
    "\n",
    "\n",
    "# def show_image_batch(sample_batched):\n",
    "#     \"\"\"Show image for a batch of samples.\"\"\"\n",
    "#     images_batch, xml_batch = sample_batched['image'], sample_batched['xml']\n",
    "#     images_batch = torch.tensor(images_batch)\n",
    "#     batch_size = len(images_batch)\n",
    "#     im_size = images_batch.size(1)\n",
    "\n",
    "#     grid = utils.make_grid(images_batch)\n",
    "#     plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "#     plt.title('Batch from dataloader')\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
