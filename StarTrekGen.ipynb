{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPOCK: Check the circuit.*\n",
      "Evacuate\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import torch\n",
    "import torchvision\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "def outerwordToTensor(sentence,all_letters): # use\n",
    "        tensor = torch.zeros([len(sentence),1,n_letters],dtype=torch.int32)\n",
    "        for word in range(len(sentence)):\n",
    "            tensor[word][0][all_letters.find(sentence[word])] = 1\n",
    "        return tensor.float()\n",
    "\n",
    "\n",
    "class star_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data,all_letters):\n",
    "        self.relevant_data = data\n",
    "        self.all_letters = all_letters\n",
    "    def wordToTensor(self,sentence): # use\n",
    "        tensor = torch.zeros([len(sentence),1,n_letters],dtype=torch.int32)\n",
    "        for word in range(len(sentence)):\n",
    "            tensor[word][0][self.all_letters.find(sentence[word])] = 1\n",
    "        return tensor.float()\n",
    "    def __getitem__(self,index):\n",
    "#         print(self.relevant_data[index])\n",
    "        return self.wordToTensor(self.relevant_data[index])\n",
    "    def __len__(self):\n",
    "        return len(self.relevant_data)\n",
    "def do_data():\n",
    "    return_data = []\n",
    "    filterwords = [\"NEXTEPISODE\"]\n",
    "    all_words = set()\n",
    "    reader = csv.reader(open(\"datasets/star_trek_transcripts_all_episodes.csv\",newline = \"\"),delimiter = \",\",quotechar ='\"')\n",
    "    for row in reader:\n",
    "        for sentence in row:\n",
    "            if sentence not in filterwords and len(sentence)>1:\n",
    "                working = sentence.strip().replace(\";\",\"\").replace('\\\"',\"\").replace(\"=\",\"\").replace(\"/\",\" \").replace('+','').replace('-',\"\").replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace(\"*\",\"\")\n",
    "                return_data.extend([working+\"*\"])\n",
    "                for i in sentence.split():\n",
    "                    all_words.add(i) # in case.. i suppose\n",
    "    return return_data, all_words\n",
    "    \n",
    "\n",
    "working_data,all_words= do_data()\n",
    "n_samples = len(working_data)\n",
    "all_letters = string.ascii_letters + \"0123456789 .,:!?()*\" #* is the EOS token.\n",
    "n_letters = len(all_letters)\n",
    "print(working_data[0]) # check that this returns a sentence\n",
    "print(list(all_words)[0]) #check this returns a complete word\n",
    "# print(outerwordToTensor(\"pp oo\",all_letters).shape)\n",
    "# print(outerwordToTensor(\"pp oo\",all_letters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim,device,num_layers = 2, tempy =None):\n",
    "        super(lstm,self).__init__()\n",
    "        self.device = device\n",
    "        self.lstm = torch.nn.LSTM(input_size =input_size,hidden_size =hidden_dim ,num_layers = num_layers,batch_first =True, dropout=0.1)\n",
    "        self.linear = torch.nn.Linear(hidden_dim,input_size)\n",
    "        self.hidden = hidden_dim\n",
    "        self.tempy = tempy\n",
    "        self.num_layers = num_layers\n",
    "        self.softmax = torch.nn.LogSoftmax(dim =1)\n",
    "    def forward(self, input , hidden,do_temp):\n",
    "        out1,hidden = self.lstm(input,hidden)\n",
    "#         print (out1.shape)\n",
    "        out2 = self.linear(out1)\n",
    "#         print(out2.shape)\n",
    "        if self.tempy and do_temp:\n",
    "            out2/=self.tempy\n",
    "        output = self.softmax(out2.squeeze(0).squeeze(1))### THIS IS A WEIRD THING\n",
    "        # also, current nll reads as batch size..?\n",
    "        return output, hidden\n",
    "    def initHidden(self):\n",
    "        return ([torch.zeros(self.num_layers,1, self.hidden).to(self.device),torch.zeros(self.num_layers,1, self.hidden).to(self.device)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 200\n",
    "total_epoch = 30\n",
    "learning_rate = 0.001\n",
    "momentum_mod = 0.9\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "my_model = lstm(n_letters,hidden_dim,device,n_letters,tempy=0.5)\n",
    "my_model = my_model.to(device)\n",
    "loss_function = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.SGD(my_model.parameters(), lr=learning_rate,momentum = momentum_mod)\n",
    "torch.optim.lr_scheduler.StepLR(optimizer,step_size = 1,gamma = 0.1,last_epoch =-1)\n",
    "random.shuffle(working_data)\n",
    "train = working_data[:int(len(working_data)/10*7)]\n",
    "test = working_data[int(len(working_data)/10*7):]\n",
    "train_loader = torch.utils.data.DataLoader(star_dataset(train,all_letters),batch_size =1,shuffle = True,num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(star_dataset(test,all_letters),batch_size =1,shuffle = True,num_workers=1)\n",
    "total_loss_list = []\n",
    "generateds = {}\n",
    "test_loss=[]\n",
    "accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KKAOO I      nuocooe oe ee ntr\n",
      "25437\n",
      "10902\n"
     ]
    }
   ],
   "source": [
    "def generate(start_letter='K',max_length =30):\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        input =outerwordToTensor(start_letter,all_letters)\n",
    "        output_phrase = start_letter\n",
    "        hidden = my_model.initHidden()\n",
    "        for i in range(max_length):\n",
    "            output, hidden = my_model(input.to(device),hidden,True)\n",
    "#             topv, topi = output.topk(1)\n",
    "#             topi = topi[0][0]\n",
    "            topv, topi = output.topk(1)\n",
    "            for sampled_letter in output:\n",
    "                sampled = int(torch.distributions.categorical.Categorical(torch.exp(sampled_letter)).sample())\n",
    "#                 sampled = topi\n",
    "                letter = all_letters[sampled]\n",
    "                output_phrase += letter\n",
    "            if letter==\"*\": # is end of sentence\n",
    "                break\n",
    "            input = outerwordToTensor(letter,all_letters)\n",
    "        return output_phrase[:-1]\n",
    "my_model.load_state_dict(torch.load(f\"startrekmodels/startrekgen_{0}.pt\")) # load the previous things.\n",
    "print(generate(\"K\"))\n",
    "print(len(train_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KHKRT::   ae   egt  sa i s ua \n",
      "KKRIT::: raan    s  sbtaclo t \n",
      "KCWKI::   e  na e\n",
      "KPTK::   aw ti nts ehr    i hn\n",
      "KKKOI:: s  eantye  eecsem  deo\n",
      "KRRKK:: o I  ae ti e n e   oii\n",
      "KRKIN: : ieo u    nn ao   nt  \n",
      "KIKRK:   t  iea oeunt \n",
      "KCO:A:Y  eee  s .aw\n",
      "KSOIR   te o e edo  to g d l  \n",
      "KAKROR  eYyarsoeat seetr e sne\n",
      "KRKR:O:  t oiate te  t  hg \n",
      "KKKKO::: ue  a   nh  ettt  nee\n",
      "KRKOI::I  anan    iati t cee  \n",
      "KIKOK:    r  oiatni smylniee  \n",
      "KKKIO:      t a h  eens ea .  \n",
      "KSC:O:   aerts  taa s odis ii \n",
      "K8AEO::  t, ieden tesgeah  eus\n",
      "KRKKR::   irtroco     e. ep\n",
      "KKKOO:   ia cscii  e  oe     e\n",
      "KKI:R:   bie eaaucto i   gn   \n",
      "KRKU::  Aoruo e    e.n    \n",
      "KCKRR::: he  rr cndist   dle o\n",
      "KKKCO:::  haleoha loe ade  te \n",
      "KKRTK::: oeate eo  r  e   e a\n",
      "KwRKE:::Y a e oeu   o.ldeoes r\n",
      "KpKR:::o so s soee   otr   ieo\n",
      "KKKKY::  reaeuei  u  i t  .ro \n",
      "KOCRK  :s   ealna     ons o. d\n",
      "KSKRA :L   ot r  o ituy ete  \n",
      "KSEK C:    ts aoo   e   d eia \n",
      "KIRK : :  t  to l   a otkntt t\n",
      "KKCKRI  e a  hnan r.t en he re\n",
      "KPRO:R:: When et, ea s a eooao\n",
      "KKKLO:    naetrer gea\n",
      "KKKC:::    l a  eeve t   erae \n",
      "KRAKK::  hsno  oce t ofo  ea..\n",
      "KKOKKO:   he  w a en \n",
      "KMISKA  e  u   ,ar ea a eg  on\n",
      "KKIOC::  m            no   eao\n",
      "K RRO:E: t s ct \n",
      "KGCRR::  o  nd oinatta   oa c \n",
      "KKI::: eaheee an ao o t  iehoo\n",
      "KKKKK :: e seunh tooan   tanu \n",
      "KARNO:Y v c a th o     ,  ler \n",
      "KKRE :  :a  e edt a aa gte  a \n",
      "KMKOK:  A n l    ,a o   ie ss \n",
      "KZ:CK::  e to iln u \n",
      "KCIKE:  :It  ato e  o  e   a a\n",
      "KKI:S: :    eaant  eeu u ontei\n",
      "KhIRC:: h u rs ot  h  is\n",
      "KRKCR:    tai  eo  etr  nsacrp\n",
      "KWKRYKC   e e e aan  t thtat o\n",
      "KCIC:C   ae   hS  e w l.cea td\n",
      "K:OK::  h   ae re a ele tak dn\n",
      "KKKKK:  h  u s t a  l ono i ta\n",
      "KOKII:::haa o.aoa sa  irte e a\n",
      "KKRKO: Chaot   h et  o h e ni \n",
      "KIKKC: :  st   \n",
      "KCRO:::  de ie  oat   n  osaoy\n",
      "KK( K: :n   a .nai netr a  su \n",
      "KRK:::   on  toaeid  an    n  \n",
      "KIOK:O:   s s  o \n",
      "KLRK::W     aeae  nhastt  la a\n",
      "KRKI:    e ii ti  asseyspne  y\n",
      "KGIRU:    nlen  e  wo   aane  \n",
      "KROR: :T    ror t nt o.oatti t\n",
      "KKOCC:  e  t roer?nat  eea e  \n",
      "KIR: ::: ooe onool de areee  t\n",
      "KEK:: T    ltnaenee ta t.i   t\n",
      "K:KRK:: eaon t   e  o   elon  \n",
      "KKIO :::AI is s ot o faeu o  t\n",
      "KKC:R:: :aaarp      l   an ttp\n",
      "KUKKRY:: ae  h    soCefa o   e\n",
      "KPRKO:     eannantn  non oon  \n",
      "KCIK::::ee  oa i  drw e   ee  \n",
      "KTNKE: N tt  u  h\n",
      "KKOKKK:E o s ta   it   s nairt\n",
      "KKRK : :   ee   lia      en is\n",
      "KKKCK::  ar   adpa e aeeae et \n",
      "KMKRK:  ia te  h      n  l hn \n",
      "KMAKK:: o ioi eeb  n n   an sg\n",
      "KKIUI:   hea teotnw oo atae eo\n",
      "KYKHI:  Isoa.el ae \n",
      "KRRRC::  pe  el asos\n",
      "KKKKR:    et  rt    .ose    ho\n",
      "KKKO:::W  e a n y ta,et oi   e\n",
      "KTRK:A:rNhaaroe  tc un n nao a\n",
      "KROR: : e a ee  s   nna n    a\n",
      "KKKK: A:  oooe   ltoa\n",
      "KOKKKK       ttti ue eiea h\n",
      "KKOR:I:: ith \n",
      "KA:CT::   u i uane s   d sr a \n",
      "KKCK     lee snee il asiaae te\n",
      "K  K:::Wyhli  rt    n ehe\n",
      "KDM:R:: a o   ou  saceen  eh r\n",
      "KNRI:::  eet nou  oe eosd ra h\n",
      "KRIIIR:    at   n teh et a  t \n",
      "KOKKCC:  o  ka as esl i to  g \n",
      "KKC:KC: Y t m t  ar iitri ies \n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning a new epoch\n",
      "batch done.1.3741154670715332\n",
      "batch done.0.5411343574523926\n",
      "batch done.1.3159565925598145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0e0e78a61188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mlatest_timing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mbatch_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_epoch = 0\n",
    "# if current_epoch!=0:\n",
    "#     my_model.load_state_dict(torch.load(f\"startrekgen_{current_epoch-1}.pt\")) # load the previous things.\n",
    "#     generateds = json.load(open(\"output_generations.json\",\"r\"))\n",
    "\n",
    "for epoch_num in range(current_epoch,total_epoch):\n",
    "    print(\"beginning a new epoch\")\n",
    "    total_loss =0\n",
    "    totes_counter=0\n",
    "    generateds[epoch_num] = []\n",
    "    batch_time = time.time()\n",
    "    for batch_number, values in enumerate(train_loader):\n",
    "        my_model.train()\n",
    "        my_model.zero_grad()\n",
    "        loss =0\n",
    "        latest_hidden =my_model.initHidden()\n",
    "        values = values.squeeze(0)\n",
    "        answer = torch.nonzero(values.squeeze(1))[:,1]\n",
    "#         print(answer.shape)\n",
    "#         print(values.shape)\n",
    "        values = values.to(device)\n",
    "        answer = answer.to(device)\n",
    "        output,hidden = my_model(values.view(values.shape[1],values.shape[0],values.shape[2]),latest_hidden,False)\n",
    "#         print(output.shape)\n",
    "        l = loss_function(output,answer.long())\n",
    "        total_loss+=l.item()\n",
    "        loss+=l\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        latest_timing = time.time()- batch_time\n",
    "        batch_time = time.time()\n",
    "        print(f\"batch done.{latest_timing}\")\n",
    "        \n",
    "        if batch_number%2000==0 and batch_number!=0:\n",
    "            temporarylist = [] # Generate a batch of 20\n",
    "            my_model.eval()\n",
    "            for genny in range(20):\n",
    "                temporarylist.append(generate(all_letters[random.randint(26,51)]))\n",
    "                generateds[epoch_num].append(temporarylist)\n",
    "            print(\"Generated 20 samples..\")\n",
    "            print(temporarylist)\n",
    "            batch_time = time.time() # reset the timing..\n",
    "            \n",
    "            \n",
    "        if batch_number%int(len(train_loader)/10)==0 and batch_number!= 0 :\n",
    "            totes_counter+=1\n",
    "            print(f\"{totes_counter*10}% done\")\n",
    "    \n",
    "#     losshold.backward()\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     losshold = None\n",
    "    temporarylist = []\n",
    "    for genny in range(20):\n",
    "        temporarylist.append(generate(all_letters[random.randint(26,51)]))\n",
    "        generateds[epoch_num].append(temporarylist)\n",
    "    json.dump(generateds,open(\"startrekmodels/output_generations.json\",\"w\"))\n",
    "    print(f\"epoch complete. Total loss = {total_loss}\")\n",
    "    total_loss_list.append(total_loss)\n",
    "    torch.save(my_model.state_dict(),f\"startrekmodels/startrekgen_{epoch_num}.pt\")\n",
    "    correct =0   \n",
    "    total_loss = 0\n",
    "    total =0\n",
    "    my_model.eval()\n",
    "    print(\"Beginning Evaluation.\")\n",
    "    for batch_number, values in enumerate(test_loader):\n",
    "        my_model.train()\n",
    "        my_model.zero_grad()\n",
    "        loss =0\n",
    "        latest_hidden =my_model.initHidden()\n",
    "        values = values.squeeze(0)\n",
    "        answer = torch.nonzero(values.squeeze(1))[:,1]\n",
    "#         print(answer.shape)\n",
    "#         print(values.shape)\n",
    "        values = values.to(device)\n",
    "        answer = answer.to(device)\n",
    "        output,hidden = my_model(values.view(values.shape[1],values.shape[0],values.shape[2]),latest_hidden,False)\n",
    "#         print(output.shape)\n",
    "        values = torch.nonzero(values.squeeze(0).squeeze(0))[:,1]\n",
    "        l = loss_function(output,values.long())\n",
    "        total_loss+=l.item()\n",
    "        topv, topi = output.topk(1)\n",
    "        topi = topi.to(\"cpu\")\n",
    "        values = values.to(\"cpu\")\n",
    "        for i in range(len(topi)):\n",
    "            total+=1\n",
    "            if topi[i]==values[i]:\n",
    "                correct+=1\n",
    "        if batch_number%1000 ==0 and batch_number!=0:\n",
    "            print(\"completed 1k of test batch\")\n",
    "    print(f\"Loss = {total_loss}\")\n",
    "    print(f\"Accuracies = {correct/total}\")\n",
    "    test_loss.append(total_loss)\n",
    "    accuracies.append(correct/total)\n",
    "    json.dump(test_loss,open(\"startrekmodels/test_loss.json\",\"w\"))\n",
    "    print(\"Test losses dumped.\")\n",
    "    json.dump(accuracies,open(\"startrekmodels/accuracies.json\",\"w\"))\n",
    "    print(\"Test Accuracies dumped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 71])\n",
      "torch.Size([64])\n",
      "tensor([36, 34, 43, 36, 65, 62, 37,  8,  4, 20, 19,  4, 13,  0, 13, 19, 62, 44,\n",
      "        20, 11, 20, 64, 62,  3, 14, 62, 24, 14, 20, 62,  5,  4,  4, 11, 62, 18,\n",
      "        19, 17, 14, 13,  6, 62,  4, 13, 14, 20,  6,  7, 62, 19, 14, 62, 12, 14,\n",
      "        21,  4, 62,  0,  1, 14, 20, 19, 67, 70])\n",
      "torch.Size([64, 1, 71])\n",
      "tensor([36, 34, 43, 36, 65, 62, 37,  8,  4, 20, 19,  4, 13,  0, 13, 19, 62, 44,\n",
      "        20, 11, 20, 64, 62,  3, 14, 62, 24, 14, 20, 62,  5,  4,  4, 11, 62, 18,\n",
      "        19, 17, 14, 13,  6, 62,  4, 13, 14, 20,  6,  7, 62, 19, 14, 62, 12, 14,\n",
      "        21,  4, 62,  0,  1, 14, 20, 19, 67, 70])\n",
      "tensor([[-5.0166e-02, -7.3258e+00, -7.3415e+00,  ..., -7.3051e+00,\n",
      "         -7.2219e+00, -7.2726e+00],\n",
      "        [-1.5097e-03, -1.0838e+01, -1.0916e+01,  ..., -1.0771e+01,\n",
      "         -1.0661e+01, -1.0710e+01],\n",
      "        [-9.7275e-05, -1.3605e+01, -1.3727e+01,  ..., -1.3499e+01,\n",
      "         -1.3367e+01, -1.3422e+01],\n",
      "        ...,\n",
      "        [ 0.0000e+00, -1.9204e+01, -1.9407e+01,  ..., -1.9037e+01,\n",
      "         -1.8846e+01, -1.8890e+01],\n",
      "        [ 0.0000e+00, -1.9226e+01, -1.9426e+01,  ..., -1.9057e+01,\n",
      "         -1.8866e+01, -1.8913e+01],\n",
      "        [ 0.0000e+00, -1.9253e+01, -1.9452e+01,  ..., -1.9085e+01,\n",
      "         -1.8891e+01, -1.8940e+01]], device='cuda:0',\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([36, 34, 43, 36, 65, 62, 37,  8,  4, 20, 19,  4, 13,  0, 13, 19, 62, 44,\n",
      "        20, 11, 20, 64, 62,  3, 14, 62, 24, 14, 20, 62,  5,  4,  4, 11, 62, 18,\n",
      "        19, 17, 14, 13,  6, 62,  4, 13, 14, 20,  6,  7, 62, 19, 14, 62, 12, 14,\n",
      "        21,  4, 62,  0,  1, 14, 20, 19, 67, 70])\n",
      "1602\n",
      "64\n",
      "[194.15922401461285]\n"
     ]
    }
   ],
   "source": [
    "test_loss=[]\n",
    "accuracies = []\n",
    "total = len(test_loader)\n",
    "total =0\n",
    "for batch_number, values in enumerate(test_loader):\n",
    "    my_model.train()\n",
    "    my_model.zero_grad()\n",
    "    loss =0\n",
    "    latest_hidden =my_model.initHidden()\n",
    "    values = values.squeeze(0)\n",
    "    print(values.shape)\n",
    "    answer = torch.nonzero(values.squeeze(1))[:,1]\n",
    "    print(answer.shape)\n",
    "    print(answer)\n",
    "    print(values.shape)\n",
    "    values = values.to(device)\n",
    "    answer = answer.to(device)\n",
    "    output,hidden = my_model(values.view(values.shape[1],values.shape[0],values.shape[2]),latest_hidden,False)\n",
    "#         print(output.shape)\n",
    "    values = torch.nonzero(values.squeeze(1))[:,1]\n",
    "    l = loss_function(output,answer.to(device))\n",
    "    total_loss+=l.item()\n",
    "    topv, topi = output.topk(1)\n",
    "    topi = topi.to(\"cpu\")\n",
    "    values = values.to(\"cpu\")\n",
    "    answer = answer.to(\"cpu\")\n",
    "    for i in range(len(topi)):\n",
    "        total+=1\n",
    "        if topi[i]==answer[i]:\n",
    "            correct+=1\n",
    "    if batch_number%1000 ==0 and batch_number!=0:\n",
    "        print(\"completed 1k of test batch\")\n",
    "    print(answer)\n",
    "    print(output)\n",
    "    print(values)\n",
    "    break\n",
    "    \n",
    "test_loss.append(total_loss)\n",
    "accuracies.append(correct/total)\n",
    "print(correct)\n",
    "print(total)\n",
    "print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total =0\n",
    "# my_model.eval()\n",
    "# for batch_number, values in enumerate(test_loader):\n",
    "#     output,hidden = my_model(values.squeeze(0).to(device),my_model.initHidden(),False)\n",
    "#     values = torch.nonzero(values.squeeze(0).squeeze(0))[:,1]\n",
    "#     l = loss_function(output,values.long().to(device))\n",
    "#     total_loss+=l.item()\n",
    "#     topv, topi = output.topk(1)\n",
    "#     topi = topi.to(\"cpu\")\n",
    "#     values = values.to(\"cpu\")\n",
    "#     for i in range(len(topi)):\n",
    "#         total+=1\n",
    "#         if topi[i]==values[i]:\n",
    "#             correct+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loss.append(total_loss)\n",
    "# accuracies.append(correct/total)\n",
    "# json.dump(test_loss,open(\"test_loss.json\",\"w\"))\n",
    "# json.dump(accuracies,open(\"accuracies.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
